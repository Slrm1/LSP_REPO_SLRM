# Assignment 3 Reflection: Object-Oriented Redesign of ETL Pipeline

The main difference between Assignment 2 and Assignment 3 can be seen when examining the way they are constructed and organized. While Assignment 2 relies on a monolithic design, with a single class handling all operations, Assignment 3 introduces a modular and object-oriented approach with a separate concern-based design.

The monolithic design used in Assignment 2 means that a single class, ETLPipeline, controls all operations. This includes configuration constants like file paths and headers, file input/output operations, processing statistics, CSV parsing, transformation rules, and the final formatting. While this design works well for a small-scale ETL pipeline, this design makes the program difficult to read, maintain, and modify. Any changes to the program, whether to the transformation rules or the input format, require changes to this class and increase the likelihood of new bugs.

In contrast, the design used for Assignment 3 separates the program into many classes, each with a specific and well-defined role. The ETLApp class handles the entry point and controls the flow between classes. The ETLConfiguration class stores configuration variables like file paths and headers. The ETLStatistics class stores statistics about the rows processed, including the number read, transformed, and skipped. The CsvETLRunner class handles the flow between the reader and writer classes. The RowProcessor class handles each row separately by calling the RowParser and ProductTransformer classes. Data is stored separately and takes the form of a ProductRecord and a TransformedProductRecord. The design used for Assignment 3 makes the program easy to read, maintain, and modify. Each class performs a well-defined role, and their interaction with each other is well-structured.

In Assignment 3, we get a better look at the object-oriented skeleton of the system by considering it as a collection of cooperating objects. We can recognize the basic idea of classes and objects by noticing how we divide responsibilities between different objects instead of loading everything into a single class. Encapsulation is nicely used, especially in ETLStatistics, where counters are kept private and change hands only through public methods like increment. In ProductRecord and TransformedProductRecord, we keep our internal states hidden by making our fields private and final, which preserves immutability and thus maintains the integrity of our objects. We favor composition over inheritance, as we construct our objects from smaller, more specialized objects, with ETLApp acting as glue for our pipeline and RowProcessor consisting of a parser and a transformer. Even though we do not use inheritance directly, we provide ourselves with opportunities for extending our system using polymorphism. We could, for example, use an interface to allow for different row processor implementations for different file types (CSV, JSON) without changing our existing objects.

To verify that Assignment 3 maintains the functional behavior of Assignment 2, a proper test plan should be implemented. To do this, a key approach would be to conduct a golden file test. This would involve running Assignment 2 with a standard input file and obtaining the output from this run. This would serve as a reference file. Next, Assignment 3 would be run with the same input file, and a comparison would be made between the outputs, including content, formatting, and order, using a file comparison tool. Additionally, a comparison would be made with regard to console outputs to ensure that statistics such as rows read, rows transformed, and rows skipped match exactly. If the input file does not exist, then it should be clear that Assignment 2 and Assignment 3 would behave in exactly the same manner and would report an error message and exit cleanly without creating any output file. If there are blank lines, improper rows, and improper numbers in the input file, then Assignment 2 and Assignment 3 should behave exactly in the same manner and skip these rows, thus incrementing the skipped counter. If Assignment 3 is run with an empty input file, then Assignment 3 should produce an output file with only the header and report zero rows processed.

In other words, Assignment 3 is a resounding success as a refactor of Assignment 2. It retains the same external behavior and functional requirements, but the internal structure is a real improvement. By increasing cohesion, decreasing coupling, and emphasizing encapsulation and composition, the object-oriented design is more maintainable, extendible, and scalable. By moving away from a monolithic design to a more modular system, a cleaner, more professional solution is achieved without altering the external behavior.
